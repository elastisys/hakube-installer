#!/bin/bash

set -e

{% if cluster["hooks"]["preinstall"] %}
{% include "hooks/preinstall/" + cluster["hooks"]["preinstall"] %}
{% endif %}

{% include "fragments/functions.sh.j2" %}

{% include "fragments/sysadm-utils.sh.j2" %}

log INFO "installing docker ..."
{% include "fragments/docker.sh.j2" %}

log INFO "installing kubeadm ..."
{% include "fragments/kubeadm.sh.j2" %}

# install ssh keys
sudo tee $HOME/.ssh/id_rsa.pub > /dev/null <<EOF
{% include cluster["sshKeysDir"] + "/" + worker_name + "_rsa.pub" %}
EOF
sudo tee $HOME/.ssh/id_rsa > /dev/null <<EOF
{% include cluster["sshKeysDir"] + "/" + worker_name + "_rsa" %}
EOF
sudo chmod 600 $HOME/.ssh/id_rsa

#
# Run kubeadm
#
cat > kubeadm-config.yaml <<EOL
apiVersion: kubeadm.k8s.io/v1alpha1
kind: NodeConfiguration
nodeName: {{ worker_name }}
token: {{ cluster_token }}
discoveryTokenUnsafeSkipCAVerification: true
discoveryTokenAPIServers:
- {{ cluster["masterLoadBalancerAddress"] }}:6443
EOL

# will wait (indefinitely) for a master to become available
sudo kubeadm join --config=kubeadm-config.yaml


#
# On the worker, kube-proxy and kubelet needs to connect to apiserver through
# the load-balancer. The configmap that configures the kube-proxy DaemonSet
# should already have been updated on master init, but we need to re-write the
# kubelet config to use the load-balancer (instead of the advertised IP of the
# master that it happened to join).
#
# /etc/kubernetes/kubelet.conf may not be immediately present
wait_for 60 [ -f /etc/kubernetes/kubelet.conf ]
sudo sed -i 's#server:.*#server: https://{{ cluster["masterLoadBalancerAddress"] }}:6443#g' /etc/kubernetes/kubelet.conf
sudo systemctl restart kubelet

log INFO "bootscript completed."
